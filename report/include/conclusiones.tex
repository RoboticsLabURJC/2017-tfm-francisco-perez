% CREATED BY DAVID FRISK, 2018
\chapter{Conclusiones y trabajos futuros}

Como último capítulo para cerrar este Trabajo de Fin de Máster, a continuación se presentan las conclusiones alcanzadas, además de evaluar los resultados obtenidos en función de los hitos marcados al principio del mismo. Por último se dedicará un último punto a mencionar las posibles líneas futuras de investigación que este proyecto abre.

\section{Conclusiones}

Antes de la realización de este proyecto se acordó que el objetivo principal era el desarrollo de un algoritmo de control visual basado en \textit{deep learning} para resolver el problema de la conducción autónoma en robots reales. Como objetivo adicional, se optó por el desarrollo de una plataforma \textit{software} para la ejecución de comportamientos complejos en los robots bautizada BehavioStudio, que fuera compatible con entornos de ejecución simulados y reales.

Tras este trabajo se dispone de esa plataforma estable, flexible y robusta para probar algoritmos de comportamientos complejos en robots, además de dos modelos basados en redes neuronales que solucionan el problema de la conducción autónoma a partir de datos visuales en el robot real JetBot. Estos dos puntos son hitos importantes tanto para el desarrollo personal, como para la organización de \textit{software} libre JdeRobot, ya que ahora dispone de una plataforma para probar algoritmos de este tipo que podrá ser aprovechada por estudiantes de máster y doctorado. El trabajo previo en el que se basa este mismo \cite{vanessa}, exploró de forma exhaustiva el comportamiento de diferentes modelos neuronales en conducción autónoma con éxito, pero se limitó al entorno de simulación. El trabajo aquí descrito amplía ese estudio llevando a cabo un estudio similar pero sobre robots reales.

De los dos objetivos principales de este proyecto, el más complejo ha sido el de solucionar el problema de la conducción autónoma, ya que ha requerido un estudio profundo de muchas de las técnicas existentes para controladores visuales, además de incluir la dificultad de hacer que el sistema desarrollado funcionara en un procesador de limitados recursos como es la Jetson Nano. Nos encontramos con infinidad de problemas tanto a nivel de \textit{software} como a nivel de \textit{hardware} para conseguir un modelo robusto que solucionara la conducción autónoma, ya que los entornos simulados ofrecen reproducibilidad exacta de los algoritmos desarrollados, pero en entornos reales hay muchos factores externos que pueden afectar a la hora de hacer pruebas. Como se ha mencionado, se han encontrado problemas diversos en cuanto a capacidad computacional, problemas con la potencia de los motores, problemas con las pistas utilizadas, iluminación, etc.; esto ha supuesto todo un reto y muchas horas de ensayo-error hasta conseguir los resultados deseados. A nivel de \textit{software}, afortunadamente los problemas han sido menos. Debido al uso de \textit{transfer learning} ya se partía de una base sólida de los modelos neuronales utilizados, aunque igualmente ha habido que ajustarlos finamente. La parte más tediosa de este objetivo fue la grabación y el etiquetado de los conjuntos de datos utilizados, ya que cada imagen se grabó y etiquetó a mano, lo que ha consumido bastante tiempo.

En cuanto al desarrollo del controlador visual basado en \textit{deep learning}, dado que las técnicas más modernas de entrenamiento permiten que los entrenamientos de los modelos no requieran de mucho ajuste (gracias a la técnica de \textit{transfer learning} sobretodo), no ha supuesto demasiada complicación. El mayor reto ha estado en ajustar los parámetros hasta conseguir el resultado deseado. Además, gracias al estudio previo de Vanessa Fernández, se ha facilitado la toma de decisiones en algún frente basándonos en sus resultados. 

Por lo que respecta al robot JetBot, la decisión de adquirir un kit de ensamblaje fue acertada, ya que ahorró gran cantidad de trabajo de diseño de piezas y montaje con impresoras 3D involucradas. El kit adquirido es sencillo de montar y dispone de todas las piezas necesarias para tener un robot funcional. No obstante, el mayor obstáculo han sido los motores que vienen de base con el kit. Al tratarse de motores genéricos de juguete, el par motor era muy bajo, lo que impedía al robot moverse a velocidades muy bajas. Este hecho ha impedido realizar un ajuste muy fino del movimiento del robot sobre los circuitos, ya que al aumentar arificialmente la velocidad se perdía control sobre el movimiento, lo que provocaba que las trayectorias del robot fueran erráticas. Además, uno de los motores (el derecho) estaba defectuoso. A igualdad de potencia, el motor derecho ofrecía menos revoluciones, por lo que aplicar una potencia del 60\% a ambos motores al mismo tiempo hacia que el robot tuviera deriva hacia la derecha, al girar el motor derecho más lento que el izquierdo. Este desajuste ha influido negativamente en las soluciones propuestas, ya que ha hecho falta un ajuste de las ganancias de ambos motores para cada experimento realizado, llegando en ocasiones a no poder completar circuitos. No obstante, este problema se soluciona fácilmente adquiriendo motores de más calidad.

Por otro lado, en cuanto al desarrollo de la plataforma \textit{software} BehaviorStudio, lo más costoso fue dar con un diseño que soportara toda la funcionalidad que se quería desde un principio. Se cuidó mucho el diseño de las abstracciones, de tal modo que el \textit{software} resultante tuviera alta cohesión y bajo acoplamiento, lo que hace la plataforma fácil de mantener. Sin embargo, a pesar de haber cuidado la etapa de diseño, surgieron también dificultades durante el desarrollo, que no se habían previsto. Por ejemplo, una de las librerías utilizadas para renderizar componentes en 3D no era compatible con la versión de Python que se utilizó inicialmente para el desarrollo, por lo que hubo que cambiar una gran parte del código para superar esa dificultad. Además, el diseño del GUI (\textit{Graphic User Interface}) fue la parte que más tiempo consumió, ya que, a pesar de tener experiencia previa con la biblioteca Qt, la gran cantidad de elementos necesarios para cubrir toda la funcionalidad requirió de muchas horas de diseño y pruebas.

La parte más compleja del desarrollo \textit{software} fue hacer que los cerebros fueran intercambiables en tiempo de ejecución. Esta es una de las características más importante de BehaviorStudio, ya que permite que la depuración de los algoritmos desarrollados sea instantánea evitando los tiempos de cerrar y volver a abrir la aplicación con las cargas que eso conlleva, sobretodo si se trabaja en entornos simulados. La capacidad que tiene la plataforma de adaptarse en caliente a cambios en los diferentes cerebros abre la puerta a que los usuarios puedan modificar los cerebros de sus robots casi en tiempo real. Esta es la piedra angular de BehaviorStudio, ya que su cometido principal es facilitar las pruebas de los algoritmos de los usuarios, y poder ver reflejados los cambios rápidamente hace que la depuración de los cerebros sea más sencilla y sobretodo más cómoda para el usuario final.

Otro reto importante fue hacer que BehaviorStudio fuera generalista, ya que a pesar de que está centrada en el problema de la conducción autónoma, siempre se tuvo en mente que a medio o largo plazo diera soporte a diferentes tipos de robots como drones, o incluso humanoides, para diferentes tipos de comportamiento. El encajar todas las piezas \textit{software} supuso un pequeño reto que fue llevado a cabo satisfactoriamente ya que se probó la plataforma con diferentes robots simulados y funcionó. Únicamente falta la validación con robots reales variados. Esta decisión se tomó principalmente para cubrir todo el espectro de robots con los que se trabaja en JdeRobot, para que más usuarios pudieran benficiarse de la plataforma.

Como recapitulación de las novedades aportadas en este trabajo, ahora se dispone de una plataforma \textit{software} completamente funcional para la ejecución de algoritmos basados en redes neuronales que gobiernen diferentes tipos de robots. Además, se ha demostrado que se puede obtener un controlador visual basado en aprendizaje profundo para solucionar el problema de la conducción autónoma en robots reales con toda la problemática que ello conlleva.

Para finalizar esta sección, dejaremos constancia de lo aprendido durante el desarrollo de este trabajo. Este proyecto en concreto implica el conocimiento de multitud de tecnologías muy diferentes entre sí, tanto \textit{software} como \textit{hardware}. Tener una base sólida en el conocimiento de las redes neuronales aplicadas a la visión ha sido clave para la consecución de los objetivos de este proyecto. Además de los conocimientos previos de alguna de las tecnologías utilizadas, ha sido necesario familiarizarse con las librerías de Pytorch, de Qt3D, y estudiar el estado actual del \textit{hardware} embebido más potente para este tipo de proyectos.

\section{Trabajos futuros}

Puesto que no es posible abarcar todas las funcionalidades que ofrece un proyecto de estas características, en este trabajo nos hemos centrado en resolver el problema de la conducción autónoma con un controlador visual basado en \textit{deep learning} sin explorar otras vías muy interesantes. Con los objetivos conseguidos, se abren multitud de posibilidades de desarrollo y líneas de investigación, utilizaremos esta sección para comentarlos.

En cuanto a la plataforma BehaviorStudio, el desarrollo de esta primera iteración ha dejado en el tintero elementos muy interesantes por explorar. Por ejemplo, la inclusión de un módulo evaluador de comportamientos que ofreciera en tiempo real métricas de cómo de bien o de mal lo está haciendo un modelo comparado con otros de referencia, o incluso con modificaciones del mismo. Esto abre la puerta a otros proyectos interesantes como la posibilidad de crear \textit{pipelines} de pruebas en los que puedas programar ejecuciones con diferentes modelos y sea la plataforma la que te devuelva las métricas de calidad de los mismos.

Además se puede incluir la posibilidad de ampliar la gama de sensores y actuadores soportados por la plataforma, obteniendo un centro de mando completo con el que conocer todos los datos sensoriales que ofrece nuestro robot. Se puede ampliar la capacidad de conectarse a diferentes tipos de robots a través del interfaz de usuario, realizar una versión web de la capa de visualización para que la plataforma pueda ser desplegada como servicio web, incluir soporte para otros simuladores más realistas o dedicados, etc.

En lo que respecta al controlador visual, se abren líneas de investigación muy interesantes. En este trabajo nos hemos centrado en resolver el problema como un problema de regresión supervisado utilizando redes neuronales convolucionales. No obstante, se pueden investigar otras vías como incluir las normas de circulación poniendo señales de tráfico en la trayectoria del robot, utilizar varios modelos simultáneamente que se encarguen de diferentes tareas (pilotar, evitar obstáculos, detectar señales de tráfico, etc.), aplicar aprendizaje por refuerzo en lugar de aprendizaje supervisado, incluir temporalidad en la toma de decisiones a través de arquitecturas más complejas como LSTMs o incluso los nuevos Transformers, eliminar las pistas haciendo que el robot navegue por espacios interiores como casas o naves, incluir sensores adicionales como LIDAR para implementar algoritmos de SLAM, y muchas otras.

Por último, y en línea de optimización del proyecto, la conexión entre los robots y la plataforma de ejecución se ha llevado a cabo utilizando la versión \textit{Melodic Morenia} de ROS, pudiendo explorarse la inclusión de ROS2 tanto en BehavioStudio como en el robot real. 